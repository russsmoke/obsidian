
```dataview 	
TABLE without id	
file.outlinks AS "OUTGOING", 	
file.inlinks AS "BACKLINKS"	
WHERE file.name = this.file.name 
```

#reference

# Links

# Description
## Данные задают потолок качества модели
1) **Разные данные приводят к разным результатам**: одна и та же модель может показывать разные результаты на разных наборах данных
2) **Качество данных устанавливает верхнюю границу**: определяет максимальную производительность, которую может достичь модель
3) **Шум и ошибки искажают обучение**: могут значительно исказить процесс обучения
4) **Непредставительные данные ломают качество**: могут привести к снижению производительности в реальных сценариях
5) **Несовпадений распределений снижает информативность**: несовпадение между обучающими и производственными данными делает метрики менее значимыми.
6) **Инженеры по ИИ работают с данными и признаками**: они должны сосредоточится не только на коде модели, но и на данных
Представим задачу кредитного скоринга. Если у данных не информации о доходах, нет признаков отражающих поведение клиентов, то даже самая сложная модель будет угадывать с потолка. Если богатый набор признаков, то самая простая модель будет работать очень хорошо.
## Жизненный цикл ИИ-системы с фокусом на данные и признаки
![[Pasted image 20251126131710.png]]
1) Постановка задачи и бизнес-требований.
   ![[Pasted image 20251126131829.png]]
   Мы формулируем, что хотим оптимизировать, какое решение должно предсказыать модель, ограничение по времени, рискам.
2) Требование к данным и метрикам.
   ![[Pasted image 20251126131944.png]]
   Решаем, какие переменные нужны, какие метрики качества будут достаточными для запуска в прод.
3) Сбор и доступ к данным
   ![[Pasted image 20251126132115.png]]
   Этап, где договариваемся с владельцами источников, настраиваем выгрузки, обращаемся к базам, складываем в управляемое хранилище. 
4) Очистка и валидация
   ![[Pasted image 20251126132225.png]]
   Приводим данные к единому формату, исправляем ошибки, проверяем ограничения
5) Подготовка признаков
   ![[Pasted image 20251126132344.png]]
   Строим признаки которые модель может переварить.
6) Обучение и оценка моделей
   ![[Pasted image 20251126132457.png]]
   Только после подготовки всех данных для моделей начинается их обучение. Уже здесь качество проделанной работы с данными показывает есть ли смысл работать с гиперпараметрами и оценивать модель.
7) Интеграция и эксплуатация в прод
   ![[Pasted image 20251126132640.png]]
   Оборачивается в сервис, встраивается в бизнес процессы.
8) Мониторинг данных и модели
   ![[Pasted image 20251126132803.png]]
   После интеграции в прод начинается цикл, где происходит мониторинг за распределениями данных, ответы моделей, не появилось ли новые типы значений, нет ли пропусков. 
   **Мониторинг в проде - это не только про метрики модели, но и про качество и распределение данных.** 
   
## Основные типы источников данных для ИИ  
1) Транзакционные системы (OLTP-БД): оперативные записи о действиях пользователей и бизнес-событиях. Регистрационные журналы, CRM, пилинги, учетные системы. Хранятся конкретные действия (регистрация, отмена заказа, маленькие операции). Не под тяжелую аналитику. Там есть транзакции, но нет удобной структуры для признаков.
2) Аналитические хранилища и витрины (DWH, marts): агрегированные и очищенные данные для аналитики. Специально подготовленные наборы данных, которые прошли очистку. Если нужны максимально сырые события, нужно смотреть другие типы.
3) Логи и потоки событий: технические и бизнес-события: клики, метрики, запросы, телеметрия. Записывается реальные поведение системы и пользователи. Для ИИ-задач - золотой источник динамики, в нем есть паттерны, аномальные события, но есть много дубликатов, шума и странных значений.
4) Файловые выгрузки и обмены: CSV/Excel/Parquet, загружаемые вручную или по расписанию. 
5) Внешние API и сторонние сервисы: геоданные, погода, курсы, контрагенты, открытые датасеты. Дополнительный источник признаков. 
6) Стриминговые платформы (Kafka, очереди и т.п.): непрерывные потоки данных для онлайн-обработки. Модель должна сразу реагировать на новые данные.
В реальных проектах не бывает только использование одного источника. 
При проектировании ИИ-системы нужно четко понимать какие источники есть, как к ним подключатся какие ограничения по объему и качеству. Откуда пришло каждое поле и учитывать специфику каждого источника.
## Мини-кейс: какие данные есть у онлайн-сервиса такси
![[Pasted image 20251127172044.png]]
1) Заказы и поездки: расстояние, длительность, день недели, регулярность поездки клиента. Некоторое количество признаков
2) Пользователи и водители: рейтинг, количество жалоб и отмен. Это все создает признаки. Часть этих данных живет в транзакционных таблицах, часть в витринах, некоторые отдельно. Мы сразу выходим на ситуацию, что когда много источников, но один ml-проект.
3) Геоданныe и дорожная обстановка: специализированные источники - карты, уникальные маршруты. Прогноз ETL.
4) Платежи и биллинг: критичные данные, важны для задач ценообразования и мошенничества.
5) Технические логи и телеметрия
В ии-системе приходится комбинировать источники, качество ваших признаков зависит от того, как сшивать данные между собой.
## Сравнение источников данных по их свойствам
![[Pasted image 20251127173555.png]]
- Разные источники дают **разные компромиссы** между детализацией, стабильностью и скоростью
- Для ИИ-проекта важно заранее понимать, что берем "как есть", а что требует серьезной подготовки.
## Что такое качество данных в инженерном смысле
![[Pasted image 20251127174412.png]]
Очень часто в проектах можно услышать: "нормальные данные, мусор небольшой есть, но не критично". Но она не помогает принять решение и спланировать работу.
Качество данных - вектор по нескольким измерениям, каждый из которых мы можем проверять автоматически.
1) Полнота - насколько сильно данные дырявые. Для задач ии полнота особенно важна, потому что пропуски могут быть не случайными. Например, некоторые пользователи с определенными типами устройств чаще выпадают из логов, и модель начинает видеть искаженную картину мира. Поэтому нужно знать где пропуски и в каком количестве.
2) Целостность - данные не противоречат сами себе. 
3) Уникальность - нет дубликатов.
4) Актуальность - нет устаревших данных, высокая частота обновления
5) Валидность - вопрос допустимых значений (нет отрицательных возрастов)
6) Точность - насколько данные отражают реальную ситуацию.
	![[Pasted image 20251128094302.png]]
## Пример грязного датафрейма и его проблемы
![[Pasted image 20251128094345.png]]
1) Дублирующая строка order_id = 101 (нарушение **уникальности**)
2) user_id и amount равны null (нарушение **полноты**)
3) age = -5 и amount = 500000 (нарушение **валидности** и возможно **точности**)
4) статус ??? и сумма N/A (нарушение **валидности**)
5) Дата 2025-13-40 99:99:00 и 2027-01-01 (нарушение **Валидности** дат и времени)
Прежде чем начинать разработки алгоритма и подбора модели, нужно посмотреть на данные и правильно их формализовать.
## Как дефекты данных ломают обучение и инференс


> [!NOTE] ИНФЕРЕНС
> процесс применения уже обученной модели к новым данным для получения предсказаний и выводов

![[Pasted image 20251128102220.png]]
1) Целевая переменная размечена с ошибками, то модель учится на неправдивых данных. 
2) Систематический шум (мусор), границы между классами размываются. Модель вынуждена тратить емкость на попытки подстроиться под хаотичные отклонения, вместо того чтобы выучить устойчивые паттерны.
3) Неправильные признаки. Модель учит артефакты сбора данных, а не закономерности мира.
4) Несовпадение train/prod. Если в обучающей выборке нет частных случаев, то модель о них не знает
5) Трудности интерпретации. Неправильные данные делают модель непрозрачной. Если не уверены в корректности признаков и таргетов, и любые объяснения важности признаков и локальных предсказаний будут вызывать недоверие.
## Уровни проверки данных: от "на глаз" до автоматизации
1) Уровень 1: Быстрый визуальный осмотр данных, разовый графики и выборки. Выводы нигде формально не фиксируются. 
2) Уровень 2: Есть список "что посмотреть" (пропуски, дубли, диапазоны), но проверки выполняются вручную перед проектом. Ручной чек-лист. Минусы - человеческий фактор.
3) Уровень 3: Набор скриптов/функций или правил, которые запускаются автоматически при обновлении датасета или перед обучением. Фиксируем правила в коде.
4) Уровень 4: Проверки встроены в конвейер данных, есть пороги, алерты, отчеты (сопровождается мониторингом). Отслеживается качество и в обучении и в проде.
## Простейшие проверки качества в Pandas-подобном коде
```
import pandas as pd
df = pd.read_csv("rides.csv")
problems = {}
problems["dup_orders"] = df["order_id"].duplicated().sum()
problems["null_user_or_amount"] = df["user_id"].isna().sum() + df["amount"].isna().sum()
problems["invalid_age"] = ((df["age"] < 0) | (df["age"]>110)).sum()
problems["invalid_amount"] = ((df["amount"] <= 0) | (df["amount"] > 100000)).sum()

allowed_status = {"created", "paid", "cancelled"}
problems["invalid_status"] = (~df["status"].isin(allowed_status)).sum()

report = pd.Series(problems, name = "n_bad_rows")
print(report)
```
## "Validation as Code": проверки как полноценный артефакт
Идея: все что вы считаете важным проверять в данных, должно быть не в голове конкретного человека и не в договоренности, а в виде явных воспроизводимых правил, которые оформлены в коде. Есть конкретные проверки, которые можно запустить в любой момент. (Декларативный YAML файл, конфигурация фреймворка). Хранятся в том же репозитории, где и скрипты. На них можно делать CodeReview, к ним можно добавить комментарии и тесты. 
1) Проверки качества данных. Становятся кодом, а не разовыми действиями в голове или в блокноте.
2) Хранение кода проверок. Хранится в репозитории рядом с пайплайнами данных и моделями, проходит ревью.
3) Результаты валидации как артефакт. Отчеты, логи, метрики, по которым можно судить о здоровье данных.
4) Явные правила валидации. Диапазоны, уникальность, обязательные поля, допустимые категории.
5) Автоматический запуск проверок. Можно встроить в разные точки жизненного цикла. 
6) Фундамент DataOps/MLOps. Без него работа моделей превращается в лотерею. 
## Пайплайн проверки и приемки данных перед обучением
![[Pasted image 20251129202344.png]]
Целостный пайплайн.
1) ![[Pasted image 20251129202422.png]]
   Представлены источники данных, а затем механизм выгрузки и копирования данных в среду. Регулярная выгрузка из базы, задачи WareFlow, Скрипт, который забирает данные из хранилища или запрос к внешнему API. Ключевая мысль - после этого шага все данные кладутся в слой data/row.
2) ![[Pasted image 20251129203617.png]]
   Мы ничего не улучшаем, не чистим. Мы просто сохраняем то, что пришло. Чтобы всегда был артефакт сырой реальности.
3) ![[Pasted image 20251129203717.png]]
   Набор проверок (Validation as code). Результаты валидации попадают в отчет (какие проверки прошли и какие провалились). Сумеем замечать тренды и деградации. Потом в gate(шлагбаум качества). Закодированы критерии приемки (доля пропусков в критичных полях, нет ни одного дубля, не появилось новых категорий, объем данных не упал). Далее идем в data/processed где данные можно очищать, нормализовывать, готовить к построению признаков. Иначе идет в ветку fail. Пайплайн полностью останавливается, либо помечает выпуск как проблемный и создает задачу разбор инцидента.
4) ![[Pasted image 20251129204449.png]]
   Данные попадают в таблицы признаков для дальнейшей работы. Фитч-инжиниринг (считаем агрегаты, статистику по пользователям). 
## DataOps: инженерный подход к работе с данными
1) Определение: набор практик и культурных привычек для надежной и предсказуемой доставки данных "как продукта". Не где то там лежат таблицы, а есть понятный конвейер, который доставляет данные в нужное время в нужном формате.
2) Основные принципы: пайплайны как код, версионирование, тесты, мониторинг, прозрачные SLA на данные.
3) Фундамент для ИИ: без стабильный пайплайнов и контроля качества данных MLOps не взлетает.
4) Аналогия с DevOps: те же идеи - автоматизация, непрерывность, совместная работа Dev/Ops, только фокус на данных (сбор, проверка, трансформация).
5) Связанные роли: DataOps связывает владельцев источников, дата-инженеров, аналитиков, ML-инженеров в один цикл.
6) Уровень курса: мы не строим платформу, но думаем о своих датасетах как о маленьких продуктах DataOps системы.
## Мини-пайплайн данных до таблицы признаков
![[Pasted image 20251129205710.png]]
1) data/raw - **немодифицируемые снимки** исходных данных: что пришло снаружи в этот день/запуск
2) Блок очистки/валидации - **код**(скрипт, ноутбук) с проверками и явными шагами трансформаций
3) data/processed - очищенные и согласованные таблицы, удобные для агрегаций и джойнов
4) data/features - **стабильная таблица признаков** с понятной схемой для обучения моделей
5) Эксперименты с моделями опираются только на data/features, а не напрямую на сырой хаос.
## Базовые понятия: объект, таргет, признак
**Объект** - Единица предсказания (например, заказ такси).
**Признаки** - Набор измеримых свойств (маршрут, время, рейтинг)
**Таргет** - Целевая переменная (Время подачи и цена)

В табличных данных: **строка = объект, колонка = признак**, отдельная колонка/метка = таргет.
Признаки НЕ = необработанные поля: они могут быть рассчитаны, агрегированы, преобразованы
## Типы признаков и их представления в данных
В реальных проектах всегда будет смесь разных типов и как вы их классифицируете, такой будет выбор модели.
1) Числовые - целый и вещественные (Сумма, возраст, расстояние). Для них важно понимать масштабы и единицы измерения. *Нельзя ставить тысячные значения и доли в один датасет, так как это может повлиять на обучение*.
2) Бинарные - Да/нет, 0/1 (Есть промокод, пользователь активен). Результат инженерного решения.
3) Категориальные - Значения из конечного набора (город, тип тарифа, способ оплаты). Представлено строками и числовыми кодами. *Но для модели будем кодировать*. Важно различать категории и дискретные числа. Возраст - числовой, тариф в виде чисел - категориальный.
4) Временные - Даты/времена и функции (час суток/день недели).
5) Текстовые - Короткие тексты, теги, описания (отзывы, категории, заголовки)
6) Счетчики и агрегаты - Количество событий, средние/максимумы (по пользователю, по объекту, по району). Инженерные признаки.
## Примеры признаков для кейса онлайн-такси
1) Признаки маршрута - прямое расстояние, длина маршрута по дороге, время в пути по историческим данным.
2) Признаки клиента - стаж в сервисе, число поездок за N дней, доля отмен, средний чек, средний рейтинг.
3) Признаки окружения - плотность заказов в районе, наличие пробок, погода, спец-события (концерты, праздники).
4) Временные признаки - час суток, день недели, выходной/будний (флаг), сезон, "сколько времени до/после час пика". Принадлежность к пикам, учебный год/каникулы. 
   **Все это функции от времени, которые в сырых данных никак не выделены. Но после простых трансформаций они превращаются в важный сигнал.**
5) Признаки водителя/авто - стаж в системе, класс автомобиля, средний рейтинг, доля отмен, среднее время подачи. 
   Все эти признаки -агрегаты. Считаем статистику за период. Такие агрегаты ГОРАЗДО полезнее, чем сырые.
6) Источники признаков - заказы и поездки, профили пользователей, водителей, геосервисы, биллинг, внешние API.
Нужно предсказать: время подачи машины (объект - заказ, таргет - время подачи), итоговая стоимость поездки (таргет - цена).
**Один объект может быть с десятками признаков.**

> [!NOTE]
> Фитч-инжиниринг - это не подставить колонку данных в модель. Вы должны сознательно подумать, какие характеристики объекта важны для задачи и из каких сырых данных их можно получить.

## Типичные ошибки в работе с признаками
1) Неопределенный объект - признаки и таргет на разных уровнях. Строить признаки по пользователю, по заказу и по устройству одновременно, но таргет определен на одном уровне. И все сваливается в одну таблицу. В одной строке признаки разных сущностей. И модель учится на противоречивых данных. **Поэтому - сначала четко определить объект и потом по нему строить признаки или аккуратно агрегировать все к нему.**
2) Утечки данных (**data leakage**) - информация из будущего ОТНОСИТЕЛЬНО МОМЕНТА ПРЕДСКАЗАНИЯ загрязняет признаки. 
   При прогнозе оттока на текущий момент, использовать количество оплат за следующие 7 дней. Агрегаты по всему времени, которые также затрагивают время которое пытаемся предсказать.
3) Магические признаки - случайные комбинации колонок без смысла. В коде появляются странные комбинации, которые улучшают метрику, но никто не может объяснить что они означают.
4) Слишком сложные признаки - хитрые трансформации вместо простых. Windows функции, кросс-агрегаты. 
5) Несогласованность train/prod - разные вычисления признаков в офлайне и в проде (разный набор скриптов). Где то включили округление, где-то поменяли период агрегации.
6) Отсутствие документации - неизвестно, как вычисляются признаки.
## Карточка признака: что обязательно фиксировать
1) Имя признака: Машиннопонятное имя признака без магии. Из имени должно быть интуитивно понятно что за признак. 
2) Смысл на человеческом: Описание признака на простом человеческом языке. Одной строкой, которая объясняет признак. (Доля отмененных заказов за 30 дней).
3) Временной срез: На какой момент времени вычисляется признак. 
4) Ограничения и проверки: ожидаемый диапазон, допустимые значения, как ловим аномалии. (Не больше 500 поездок за 30 дней)
5) Уровень объекта: Уровень объекта, с которым будет строка data/features. Явно записывать к чему относится одна строка в таблице. Четкое описание.
6) Формула/алгоритм: Как признак считается из сырых данных. Какие таблицы берем, какие фильтры закладываете, какие агрегирующие функции используем. (Считать в таблице rides количество записей с статусом completed по данному user_id за интервал t = 30 дней)
7) Источник данных: Из каких таблиц/сервисов берутся сырые поля.
Такую карточку признака можно хранить в markdown, в Wiki странице, в таблице в репозитории.
## Как оформить данные и признаки в репозитории проекта
![[Pasted image 20251130080058.png]]
1) data/row - не заходим руками, чтобы подправить. Либо правим цепочку выгрузки либо код обработки, но оригинальный снимок не трогаем.
2) data/processed
3) data/features - таблицы, где каждая строка = объект, каждая колонка = признак и таргет. 
4) notebooks
5) features - есть подпапка cards в виде markdown (карточка признака)
6) docs - описание проверок, перечисление источников, принципы построения
![[Pasted image 20251130081113.png]]
