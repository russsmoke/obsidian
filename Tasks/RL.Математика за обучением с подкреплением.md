
```dataview 	
TABLE without id	
file.outlinks AS "OUTGOING", 	
file.inlinks AS "BACKLINKS"	
WHERE file.name = this.file.name 
```

#reference

# Links

# Description
Машинное обучение:
1) С учителем: регрессия, классификация
2) Без учителя: кластеризация - разделить на категории без разметки
3) С подкреплением: ставит модель в роли главного героя в игре. Где в игре много разных событий и заставляет модель претерпевать эти события и выбирать наилучшее решение

Обучение с подкреплением (Reinforcement Learning или RL) - раздел ИИ, посвященный созданию агентов, способных взаимодействовать с некоторой средой согласно политике и решать поставленную задачу, получая награду.
Агент выполняет действие А, а мир меняет свое состояние и так по кругу. И внешняя среда сообщает какую то награду агенту, тем самым давая оценку его действиям.

Агент - программа, которая учится, взаимодействия с окружающей средой.
Политика агента - стратегия, согласно которой агент принимает те или иные решения
Среда - контекст мира, в котором действует агент. Среда транслирует агенту свое состояние.
Награда - численная оценка действия агента в среде, необходима для обучения агента.

Например,
создание беспилотных автомобилей. Учатся избегать опасных ситуаций и соблюдать правила. Агент - модель, среда - лидары/камеры, политика - пдд, награда - сообщения водителя.

Игровая индустрия
RL полезен для игровой индустрии. AlphaGo использовала RL для игры в го и победы над чемпионами мира, алгоритм AlphaZero обыгрывал гроссмейстеров в шахматы. Она играла несколько тысяч часов с собой и подбирала стратегии. 
Atari - игры со старых игровых автоматов. Использовать как датасет для своих моделей.

Рекомендательные системы
Чтобы изучать опыт взаимодействия с клинетами. Это помогает компаниям улучшать качество рекомендаций по продуктам, чтобы они лучше соответствовали желаниям людей.
Например, рекомендательная система предлагает несколько вариантов дополнительных товаров к корзине клиента и на основе частых кликов по этим товарам обучается, чтобы рекомендовать лучшие сочетания к корзине.

Трейдинг бот - пример использования RL

Алгоритмическая торговля
Роботы, основанные на RL, используются в алгоритмической торговле для автоматической покупки и продажи акций в зависимости от рыночных условий. RL играет важную роль в управлении рисками, выявляя потенциальные проблемы с инвестициями и предотвращая убытки. Боты могут консультировать людей по поводу их финансовых целей.

На каждом курсе по обучению с подкреплением - написать свои крестики нолики. Чтобы их написать нужно задать правила игры, множество действий, ситуации когда игра разрешена, переход из одного состояния в другого, награды. 


Состояние - с течением времени меняется - функция, зависящая от времени. Меняется непрерывно (отвечает производная). Как меняется среда со временем. Мы говорим, что изменение состояния (функция), зависит от нынешнего состояния и когда оно происходит.

Лаплассовский детерминизм.

Опираясь на текущее состояние среды, можем определить все что будет дальше.

Когда появляется агент, то состояние зависит также от действий агента. Агент принимает действия в зависимости от состояния среды и времени.

Состояние, которую развиваем в будущем зависит от текущего состояния, времени и от действий агента.

Можем посчитать функцию штрафа - в ответ на какое то состояние и на действие которое мы совершили и на время можем посчитать эту функцию. Модель которая суммирует все штрафы должна иметь минимальный характер.

Но суммировать бесконечное количество величин это нецелесообразно. Поэтому берем интеграл (Риманна) и он должен быть минимальным.

Нужно, чтобы сумма штрафов была минимальна, а состояние будущее зависело от текущего состояния и действий. - постановка задачи
Есть два разных подхода - оптимальное управление и обучение с подкреплением.

Разница в сложности задачи. Например, в крестиках и ноликах не так много нужно вычислять. А обучение для победы над чемпионами в любой сфере это сложная задача. Здесь тогда используется RL (машинное обучение).

RL
Марковская цепь - это описание состояний (большое количество), описывает с какой вероятностью наступит состояние если мы знаем предыдущий исход. 
Пример, игра, в которой 4 состояния и мы находимся в одном из состояний. Набор состояний, по которым мы передвигаемся с какой то вероятностью между состояниями. **Вероятность прейти в другое состояние зависит от начального состояния.**

**Марковский процесс принятия решений.** (MDP). Полная постановка задачи.
Входит множество состояний, набор действий в ответ на какие то состояния, вероятности перехода из одного состояния в другое, награда - из каждого состояния.

Агент из множества действий выбирает одно. Говорит о своих действиям на основе стратегии (или политики). Это действие выбирается из состояния политики.

Мир дает ответ. На основе состояния текущего и действия дается награда. Тогда меняется среда, и состояние определяется функцией вероятности наступления нового действия, которое опирается на текущее состояния и действий агента.

Тогда среда дает знать через награду.