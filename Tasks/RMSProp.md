
```dataview 	
TABLE without id	
file.outlinks AS "OUTGOING", 	
file.inlinks AS "BACKLINKS"	
WHERE file.name = this.file.name 
```

#reference

# Links

# Description
**RMSProp (Root Mean Square Propagation)** — это продвинутый **алгоритм оптимизации**, улучшенная версия градиентного спуска, которая автоматически адаптирует **скорость обучения (η)** для каждого параметра модели.

## **Простая аналогия:**

Представьте, что вы спускаетесь с холма к долине (минимизируете ошибку).

- **Обычный градиентный спуск** — вы делаете **одинаковые шаги** во всех направлениях, даже если склон в одном направлении крутой, а в другом — пологий.
- **RMSProp** — вы **адаптируете размер шага**:  
  — Если параметр сильно "скакал" (большие градиенты в прошлом) → делаем **меньший шаг** (чтобы не перепрыгнуть минимум).  
  — Если параметр менялся плавно (малые градиенты) → делаем **больший шаг** (чтобы ускориться).

---

## **Как работает RMSProp (шаги):**

Для каждого параметра \( w_i \) мы храним **скользящее среднее квадратов градиентов** \( v_i \).

### **Формулы:**

1. **Вычисляем градиент** gt​=∇E(wt​) на шаге \( t \).
2. **Обновляем скользящее среднее квадратов градиентов:**
$$
   v_t = \beta v_{t-1} + (1 - \beta) g_t^2
$$
   Здесь:
   - $g_t^2$ — поэлементный квадрат градиента.
   - β — коэффициент затухания (обычно ~0.9), определяет, насколько сильно мы учитываем прошлые градиенты.
3. **Обновляем параметры:**
$$
   w_{t+1} = w_t - \frac{\eta}{\sqrt{v_t} + \epsilon} \cdot g_t
$$
   Здесь:
   - $\eta$ — глобальная скорость обучения.
   - $\sqrt{v_t}$  — корень из скользящего среднего квадратов градиентов (RMS — root mean square).
   - $\epsilon$ — маленькое число (~1e-8), чтобы избежать деления на ноль.

---

## **Ключевая идея:**

- $v_t$ **оценивает величину (мощность) градиентов** для каждого параметра.
- Если градиенты большие и "шумные" → $v_t$ большое → шаг уменьшается ($\eta / \sqrt{v_t}$ маленький).
- Если градиенты маленькие и устойчивые → $v_t$ маленькое → шаг увеличивается.

**Таким образом, скорость обучения адаптируется для каждого параметра индивидуально.**

---

## **Зачем это нужно?**

1. **Автоматическая настройка шага:** Не нужно вручную подбирать η для каждого параметра.
2. **Работа с "оврагами" функции ошибки:** В узких оврагах градиенты колеблются (большие то в одну, то в другую сторону), RMSProp уменьшает шаг, чтобы не "прыгать" из стороны в сторону.
3. **Ускорение сходимости:** Особенно на сложных ландшафтах ошибки (нейросети).

---

## **Пример на пальцах:**

Допустим, у нас два параметра:
- \( w_1 \): его градиенты всегда около 0.1 (пологий склон).
- \( w_2 \): его градиенты скачут: +10, -8, +12 (крутой и шумный склон).

**RMSProp сделает:**
- Для \( w_1 \): \( v \) маленькое → шаг будет **больше** → быстро продвинемся.
- Для \( w_2 \): \( v \) большое → шаг будет **маленький** → осторожно, без перепрыгивания.

---

## **Сравнение с другими методами:**

- **Обычный SGD:** один η для всех параметров.
- **Momentum:** добавляет "инерцию", но не адаптирует шаг.
- **AdaGrad:** накапливает все квадраты градиентов, что со временем слишком уменьшает шаг.
- **RMSProp:** "забывает" старые градиенты (через β), что решает проблему AdaGrad.
- **Adam:** сочетает идеи Momentum и RMSProp (сейчас самый популярный).

---

## **Когда использовать RMSProp?**

- Для рекуррентных нейронных сетей (RNN) он исторически был популярен.
- В задачах с **нестационарными градиентами** (когда градиенты сильно меняются).
- Часто используется, но сейчас **Adam** чаще является выбором по умолчанию.

---

## **Итог простыми словами:**

**RMSProp — это "умный" градиентный спуск, который сам подстраивает размер шага для каждой ручки настройки, смотря на то, как сильно эта ручка раньше дергалась.**

Он делает обучение более **стабильным и быстрым** на сложных ландшафтах ошибки.

**RMSProp (Root Mean Square Propagation)** — это продвинутый **алгоритм оптимизации**, улучшенная версия градиентного спуска, которая автоматически адаптирует **скорость обучения (η)** для каждого параметра модели.
