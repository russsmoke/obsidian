## Основы линейной алгебры
Линейная алгебра - раздел математики, который изучает линии, плоскости и пространства, а также отношения между ними.

Примеры применения линейной алгебры:
- Анализ и предсказание движения автомобилей
- Работа компьютерных сетей
- Машинное обучение и анализ данных
- Оптимизация процессов на производстве
- Расчет эффективности энергосистем
- Прогнозирование погоды
- Разработка и анализ финансовых систем (кредитный скоринг)
- Исследование социальных сетей и анализ больших данных
В ML И DS:
- используется для обработки и анализа больших объемов данных
- Машинное обучение - создание алгоритмов для обучения и принятия решений на основе данных
- Глубокое обучение - подраздел машинного обучения, использующий нейронные сети. Линейная алгебра помогает работать с нейронными сетями и обрабатывать информацию в них
- Например, использование линейной алгебры для автономных авто:
  - обработка данных сенсоров: преобразование данных, объединение данных из разных источников, вычисление расстояний и углов
  - Предсказание траектории движения: вычисление скорости и направления движения, предсказание будущего положения автомобиля
  - Оптимизация маршрута: выбор наиболее эффективного пути к цели, учет дорожных условий, трафика и препятствий
  - Важность эффективной навигации: критическая роль навигации для безопасности и эффективности автономных автомобилей.

---

Основные понятия: 
- Скаляр - это число.
- Вектор - массив из n чисел. ($x \in R^4$ - Вектор, состоящий из вещественных числе длины 4. Точка в пространстве размерностью 4)
- Матрица - двумерный массив. ($A_{n,m} \in R^{n*m}$)
- Тензор - многомерный массив чисел, где число измерений называется рангом тензора. Скаляры - тензоры ранга 0, вектор - тензор ранга 1, матрица - тензор ранга 2. Тензоры более высокого ранга имеют 3 и более измерений.
Свойства транспонирования матриц:
![[Pasted image 20260202003320.png]]


---
**Линейные уравнения**
**Пример:** Компания производит высококачественные солнечные панели. У компании есть фиксированный объем затрат в месяц, 1000 у.е.
Чтобы произвести одну панель нужно потратить 50 у.е.
Продает компания солнечные панели за 100 у.е.
Составим уравнения прибыли затрат компании обозначив количество солнечных панелей за x.
![[Pasted image 20260202010114.png]]

**Система линейных уравнений**
Есть система линейных уравнений
![[Pasted image 20260202010510.png]]
Зная операции с матрицами и векторами можем записать эту систему более компактно:
![[Pasted image 20260202010546.png]]

**Пример:** Компания производит товары N1, ..., Nn. Для производства которых требуются некоторые ресурсы R1, ..., Rm.
Для производства товара N необходимо a единиц ресурса R

Цель состоит в том, чтобы найти оптимальный производственный план, т.е. план того, как много единиц x товара N должно быть произведено, если доступно всего b единиц ресурса R и не осталось неиспользованных ресурсов (в идеале).
![[Pasted image 20260202011104.png]]

**Решение систем линейных уравнений:**
Методы  решения систем линейных уравнений:
- Графический (строим графики уравнений, находим пересечение)
- Метод подстановки
- Метод исключения (сложения)
- Матричный метод
- Метод Крамера
- Метод Гаусса
Виды систем линейных уравнений:
- Совместные (есть решения, удовлетворяющие всем уравнениям)
	- одно решение
	- бесконечно много решений
- Несовместные (нет решений)

---
## Векторы в пространстве

**Линейная комбинация:**
Это «смесь» векторов, где каждый вектор умножается на какое-то число (коэффициент), а потом все результаты складываются.
![[Pasted image 20260202014010.png]]
![[Pasted image 20260202014021.png]]
**Линейная оболочка:**
Это **множество всех возможных линейных комбинаций** заданного набора векторов.
Если у вас есть набор векторов, то их линейная оболочка — это все векторы, которые можно из них «собрать», комбинируя их с разными коэффициентами. (Используя только сложение векторов и умножение на скаляр)

1) Линейная оболочка большинства пар векторов охватывает бесконечную плоскость двумерного пространства
2) Если базисные векторы лежат на одной прямой, то линейная оболочка - прямая.

- **Линейная комбинация** — это одна конкретная «точка» (вектор), полученная смешиванием.
    
- **Линейная оболочка** — это всё «пространство» (прямая, плоскость и т.д.), которое можно покрыть, используя эти векторы как «строительные блоки».

**Базис пространства** — это такой минимальный набор векторов, линейная оболочка которого равна всему пространству.


---
## Матрицы в пространстве. Линейные трансформации
**трансформация** - то же самое, что и функция. Она что-то принимает в входы и дает другие значения на выходе. Этим что-то являются **Векторы**.
**Линейная** - если она имеет два свойства:
- Все линии должны оставаться без искривлений.
- Начало координат должно оставаться на месте.
Для того, чтобы применить линейную трансформацию на векторе, нужно не менять сам вектор, а поменять его базисные векторы.

Например, у нас есть вектор v = -1i + 2j
Чтобы сделать линейную трансформацию допустим j' = \[1, -2] i' = \[3, 0] (поменяли базисные векторы, сделали поворот и сдвиг), нужно координаты вектора v умножить на новые базисные векторы, и тогда мы получим новый вектор v' = \[5, 2]

Эти изменения в базисах упаковывают в **матрицу**.
Матрицы в пространстве описывают какую-то линейную трансформацию. Она содержит столбцы, где каждый описывает как изменились базисные векторы. 

Эта матрица работает как функция: записывается сначала матрица, затем изменяемый вектор и на выходе получаем трансформированный вектор - A \* v = v'

![[Pasted image 20260204230814.png]]

Например:
1) поворот на 90 градусов против часовой стрелки: i = \[0, 1] j = \[-1, 0] 
2) сдвиг: i = \[1, 0] j = \[1, 1]
3) Пространство сплющили в линию: i и j линейно зависимы

### Композиции трансформаций
Когда нужно применить две или более трансформации пространства, то это называют композицией.
Например, нужно к вектору применить поворот и сдвиг:
1) Сначала можем умножить вектор на матрицу поворота, а получившийся вектор на матрицу сдвига
2) Однако, намного удобнее умножить вектор на матрицу (поворота и сдвига), которая получилась путем перемножения матриц
В линейной алгебре перемножение матриц - применение нескольких линейных трансформаций.
![[Pasted image 20260208170025.png]]
на картинке показано правило перемножения (читаем справа налево примененные трансформации)
Порядок умножения матриц важен

---
## Определитель
Некоторые линейные преобразования растягивают пространство, а некоторые сжимают его. Есть **мера** этого сжатия и растяжения, мера того как меняется **площадь** при применении преобразования.
Численные показатель, которые показывает насколько сильно изменилась площадь называется **определитель**.

1) det L = 0 - базисы линейно зависимы
2) det L < 0 - поменялся порядок следования базисов (поменяли ориентацию пространства)
3) det L > 0 - насколько сильно поменялось пространство
Свойства:
- $det L * A * B * C = det L * det A * det B * det C$
- $det A^T = det A$
- $det A^{-1} * det A = 1$ (единичная матрица)
Все это работает и в пространствах большей размерности, но тогда, например, в трехмерном пространстве мы смотрим не на площадь, а на объем.
---
## Симметричные матрицы
Квадратная матрица называется симметричной, если совпадает с транспонированной:
$$
A = A^T
$$

Симметричные матрицы часто возникают в машинном обучении, когда их элементы порождаются некоторой функцией от двух аргументов, которая не зависит от порядка этих двух аргументов. Чаще всего это матрицы попарных расстояний между объектами. Они используются в различных алгоритмах машинного обучения. (типичная матрица смежности в графах)

---
## Диагональные матрицы
Это матрица, у которой все элементы вне главной диагонали равны нулю. (например, единичная матрица)

Диагональную матрицу можно представить в виде вектора $diag(v)$, который будет состоять из элементов главной диагонали.

- Можно быстро и эффективно находить обратную матрицу (если все элементы на диагонали не равны нулю)
  ![[Pasted image 20260205162039.png]]
- Можно быстро и эффективно вычислять определитель: ![[Pasted image 20260205162119.png]]
  По сути диагональная матрица только растягивает или сжимает пространство вдоль базисных векторов.
- Можно быстро возводить матрицу в степень. Просто возводятся в некоторую степень элементы на главной диагонали.

---
## Нормы
В машинном обучении часто надо узнать длину вектора, для измерения длины существуют функции называемые **нормами**. В общем виде норма $L^p$ задается так:
$$
||x||_p = (\sum_i|x_i|^p)^{1/2}, p \in R, p >= 1
$$
Интуитивно норма вектора х измеряет расстояние от точки х до начала координат.

1) **Евклидова норма** ($L^2$):
   $$
   ||x||_2 = (\sum_i |x_i|^2)^{1/2}
   $$
   Является самой распространенной нормой в машинном обучении, поэтому часто ее обозначают просто $||x||$, не используя надстрочный индекс 2.
   Принято считать за длину вектора квадрат нормы $L^2$, который просто равен $||x||_2^2 = x^T*x$
2) Формула для скалярного произведения двух векторов через нормы:
   $$
   \langle x,y \rangle = ||x||_2 * ||y||_2 * cos \theta
   $$Где $\theta$ это угол между векторами
3) **Манхэттенское расстояние ($L^1$):**
   $$
   ||x||_1 = (\sum_i |x_i|)
   $$
   Сумма модулей элементов.
4) Квадрат $L^2$ нормы слишком медленно растет около нуля, поэтому часто, когда нужно различать нулевые и около-нулевые элементы, берут норму $L^1$
   Часто их комбинируют в одну функцию:
   $$
   L = \alpha L^2 + \beta l^1 = \alpha (\sum_i|x_i|^2)^{1|2} + \beta (\sum_i|x_i|)
   $$
---
## Ортогональные векторы
Ортогональные векторы - это векторы, которые расположены перпендикулярно друг другу, то есть образуют в пространстве угол 90 градусов.
Например, векторы (2,0) и (0,3) являются ортогональными, потому что они образуют прямой угол. Базисные векторы i и j также являются ортогональными.
- Скалярное произведение ортогональных векторов равно **нулю**

Формула скалярного произведения:
$$
\langle x,y \rangle = x^T * y = 0
$$
---
## Ортонормированные векторы
Ортогональные векторы, норма которых равна 1, называются ортонормированными. Единичные векторы, которые перпендикулярны друг другу.
Нормировать вектора - представлять как единичный вектор.

---
## Ортогональная матрица
Ортогональной называется квадратная матрица, строки и столбцы которой образуют ортонормированные системы векторов (каждый вектор ортогональны друг другу и их нормы равны 1). Это значит, что если мы вычислим скалярное произведение строки на саму себя, то получим 1, иначе 0.
$$
A^T * A = A * A^T = I_{единичная}
$$
Чтобы найти обратную матрицу к ортогональной нужно ее просто транспонировать.
$$
A^{-1} = A^T
$$

Самый простой пример ортогональной матрицы - это единичная матрица $I$

Определитель ортогональной матрицы равен $\pm 1$:
$$
1 = det I = det A^TA = detA^TdetA = detAdetA = (detA)^2 = 1
$$
Ортогональная матрица сохраняет углы и расстояния.
Любая матрица поворота, отражения и перестановок является ортогональной:
![[Pasted image 20260205234909.png]]

 - Сохраняет углы и расстояния
 - Упрощает вычисления
 - Более стабильные численные преобразования
 - Часто применяется в ML DL
 ---
## Собственные векторы
Такие векторы, которые не покидают свою линейную оболочку при применении матрицы L называются собственными векторами матрицы L.
Эффект, который оказывает матрица на собственные векторы - растяжение или сжатие, как будто мы умножили вектор на какой то скаляр (никаких поворотов).

*Собственный вектор матрицы - ненулевой вектор, применение к которому линейного преобразования, описанного матрицей, дает **коллинеарный** вектор - тот же вектор, умноженный на некоторый скаляр.*

Собственным вектором матрицы L называется ненулевой вектор $v$ - такой, что умножение L на $v$ изменяет лишь масштаб $v$:
$$
L * v = \lambda * v
$$
Скаляр $\lambda$ называется собственным значением, соответствующим этому собственному вектору.
$$
L * v = \lambda * v \to Lv = (\lambda I)v \to (L - \lambda I)v = 0
$$
При умножении матрицы на ненулевой вектор получается нулевой вектор. Такое может быть только когда матрица сжимает пространство и теряет весь объем, иными словами, если определитель равен 0.
$$
L * v = \lambda * v \to det(L - \lambda I) = 0
$$
- У **диагональных матриц** собственные векторы этой матрицы - все векторы из стандартного базиса: i, j, k
  А собственные значения записаны на диагонали.
---
### Разложение матриц. Спектральное разложение
**ТОЛЬКО ДЛЯ КВАДРАТНЫХ МАТРИЦ**
Любое число можно разложить на простые множители.

Спектральное разложение - разложение на основе собственных векторов и собственных значений.
Пусть квадратная матрица $A_{n,n}$ имеет n линейно независимых собственных векторов $\{v_1, ..., v_n\}$ с собственными значениями $\{\lambda_1, ..., \lambda_n \}$ 
Образуем из собственных векторов матрицу V, в которой каждый столбец - это собственный вектор : V = $\{v_1, ..., v_n\}$ 
Из собственных значений образуем диагональную матрицу $\lambda$, на главной диагонали которой будут стоять собственные числа $\{\lambda_1, ..., \lambda_n \}$. Тогда спектральное разложение матрицы А описывается формулой:
$$
A = V*\lambda*V^{-1}
$$
Доказательство:
$$
AV_k = \lambda_kV_k \to AV = V\lambda \to AVV^{-1} = V\lambda V^{-1} \to A = V\lambda V^{-1}
$$
где $V_k$ - произвольный столбец.

Для чего?
- Можем быстро и эффективно возводить такие матрицы в любую степень:
  $A^x = V \lambda^x V^{-1}$ 
- Используется в различных алгоритмах машинного обучения:
  PCA
  Спектральная кластеризация
  PageRank от Google
  CV

---
### Квадратичные формы
Пусть есть симметричная квадратная матрица $A_{n,n}$ , тогда функция вида
$$
f(x) = x^TAx
$$
называется квадратичная форма. Нас интересует как связано множество значений функции f с матрицей $A_{n,n}$ . Оказывается, что оно зависит от собственных значений матрицы $A_{n,n}$:
- Положительно определенная квадратичная форма-> все собственные значения больше нуля $\forall x: x^T A x >0$ 
- Положительно полуопределенная квадратичная форма -> все собственные значения больше или равны нулю $\forall x:x^TAx >= 0$
- Отрицательно определенная квадратичная форма -> все собственные значения меньше нуля $\forall x:x^TAx < 0$
- Отрицательно полуопределенная квадратичная форма -> все собственные значения меньше или равны нулю $\forall x:x^TAx <= 0$


Но можно не считать собственные значения, а воспользоваться **критерием Сильвестра**:

Угловой минор размера k - определитель подматрицы размера k x k
$$
\Delta_1 = a_{11}; \Delta_2 = \det{\begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{pmatrix}}; ...; \Delta_n = detA
$$
### Критерий Сильвестра
Критерий Сильвестра гласит: 
- Положительная определенность -> $\forall x: x^T A x >0$ , *когда все угловые миноры от 1 до n больше нуля.*
- Отрицательная определенность $\forall x:x^TAx < 0$, *когда все знаки угловых миноров от 1 до n чередуются, причем $\Delta_1<0$*

---
### Разложение матриц. Сингулярное разложение SVD
Разложение на сингулярные векторы и сингулярные числа.

SVD является частным случаем спектрального разложения матрицы.

Но в отличие от спектрального, сингулярное разложение есть у любой действительной матрицы, даже если она **НЕ КВАДРАТНАЯ**.

Сингулярное разложение матрицы записывается в виде произведения трех матриц:
$$
A_{m,n} = U_{m,m} *\Sigma_{m,n}*V^T_{n,n}
$$
- $A_{m,n}$ - исходная матрица
- $U_{m,m}$ - ортогональная матрица, где по столбцам левые сингулярные векторы
- $V_{n,n}$ - ортогональная матрица, где по столбцам правые сингулярные векторы
- $\Sigma_{m,n}$ - диагональная матрица, где значения это сингулярные числа
**Левые сингулярные векторы** - собственные векторы матрицы $AA^T$
**Правые сингулярные векторы** - собственные векторы матрицы $A^TA$
**Сингулярные числа** - квадратные корни из собственных значений для матрицы $A^TA$ и $AA^T$ 

Для:
- сжатие изображений 
- рекомендательные системы
- решение систем линейных уравнений

---
## Ранг
Ранг матрицы - наибольшее число ее линейно независимых строк, если **ОПРЕДЕЛИТЕЛЬ НЕ РАВЕН НУЛЮ**
Если определитель равен нулю, то нужно найти **наименьшее алгебраическое дополнение из этого определителя.**

Ранг матрицы показывает, сколько измерений стало после линейной трансформации.
Например, ранг 3 означает, что после преобразования вектор существует в трехмерном пространстве.

---
## Ядро матрицы. Нулевое пространство. Линейная оболочка
Это набор векторов, которые попали в начало координат после линейной трансформации.

---
## Неквадратные матрицы
Неквадратные матрицы показывают в какой размерности будет выходной вектор. Например, матрица 3 на 2 (3 строки, 2 столбца) показывает, что изначально было двумерное пространство с базисными векторами i и j. Затем после преобразования у базисных векторов оказалось 3 координаты, это значит пространство стало трехмерным.
Также может быть и в обратную сторону.

---
## Скалярное произведение
Скалярное произведение двух векторов - это проекция одного вектора на ось другого и умножение их длин. Тогда мы получим скаляр - число.
